"""
Comprehensive ML Pipeline - Advanced Analytics & Year-by-Year Forecasting
==========================================================================
Uses all collected data to train multiple models, generate predictions,
and perform advanced statistical analysis for the ML Insights page.

Models trained:
- Predictive: Random Forest, Gradient Boosting, XGBoost, LSTM, Ridge, Lasso, ElasticNet
- Non-predictive: K-Means, PCA, DBSCAN, Hierarchical Clustering, Isolation Forest

Outputs:
- Year-by-year predictions (2025-2030)
- Model comparison metrics
- Feature importance rankings
- Correlation matrices
- Cluster analysis results
- Anomaly detection
"""

import pandas as pd
import numpy as np
import json
import warnings
from pathlib import Path
from datetime import datetime
import os


class NumpyEncoder(json.JSONEncoder):
    """Custom JSON encoder for numpy types."""
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        return super().default(obj)

# ML imports
from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, IsolationForest
from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from scipy.stats import pearsonr, spearmanr
from scipy.signal import find_peaks

# Deep Learning
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# Try to import XGBoost
try:
    import xgboost as xgb
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False
    print("XGBoost not available, using GradientBoosting instead")

warnings.filterwarnings('ignore')

# GPU setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name()}")

# Paths
DATA_DIR = Path("data/downloaded")
OUTPUT_DIR = Path("website/src/data")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


# =============================================================================
# DATA LOADING
# =============================================================================

def load_fred_data(filename, value_col=None):
    """Load FRED CSV data and parse dates."""
    filepath = DATA_DIR / filename
    if not filepath.exists():
        return None
    
    df = pd.read_csv(filepath)
    
    # Find date column
    date_col = None
    for col in ['observation_date', 'DATE', 'date', 'Date']:
        if col in df.columns:
            date_col = col
            break
    
    if date_col is None:
        return None
    
    df['date'] = pd.to_datetime(df[date_col])
    df = df.set_index('date')
    
    # Get the value column
    if value_col is None:
        value_col = [c for c in df.columns if c != date_col][0]
    
    df = df[[value_col]].rename(columns={value_col: filename.replace('.csv', '').replace('fred_', '')})
    df = df.apply(pd.to_numeric, errors='coerce')
    
    return df


def load_all_data():
    """Load and merge all available datasets."""
    print("Loading all datasets...")
    
    datasets = {}
    
    # FRED datasets - commodities
    commodities = [
        ('fred_copper_price.csv', 'copper'),
        ('fred_aluminum_price.csv', 'aluminum'),
        ('fred_nickel_price.csv', 'nickel'),
        ('fred_zinc_price.csv', 'zinc'),
        ('fred_tin_price.csv', 'tin'),
        ('fred_lead_price.csv', 'lead'),
        ('fred_iron_ore_price.csv', 'iron_ore'),
    ]
    
    for filename, name in commodities:
        df = load_fred_data(filename)
        if df is not None:
            datasets[name] = df
            print(f"  âœ“ Loaded {name}: {len(df)} rows")
    
    # FRED datasets - economic indicators
    economic = [
        ('fred_fed_funds_rate.csv', 'fed_rate'),
        ('fred_treasury_10yr.csv', 'treasury_10yr'),
        ('fred_treasury_2yr.csv', 'treasury_2yr'),
        ('fred_cpi_all_items.csv', 'cpi'),
        ('fred_cpi_energy.csv', 'cpi_energy'),
        ('fred_industrial_production.csv', 'industrial_prod'),
        ('fred_manufacturing_production.csv', 'manufacturing'),
        ('fred_energy_index.csv', 'energy_index'),
        ('fred_us_unemployment.csv', 'unemployment'),
        ('fred_consumer_sentiment.csv', 'consumer_sentiment'),
        ('fred_gas_price_regular.csv', 'gas_price'),
        ('fred_natural_gas_eu.csv', 'natgas_eu'),
        ('fred_vehicle_sales.csv', 'vehicle_sales'),
    ]
    
    for filename, name in economic:
        df = load_fred_data(filename)
        if df is not None:
            datasets[name] = df
            print(f"  âœ“ Loaded {name}: {len(df)} rows")
    
    # Merge all datasets
    if not datasets:
        raise ValueError("No datasets loaded!")
    
    # Start with the first dataset
    merged = None
    for name, df in datasets.items():
        if merged is None:
            merged = df
        else:
            merged = merged.join(df, how='outer')
    
    # Resample to monthly and forward fill
    merged = merged.resample('ME').last()
    merged = merged.ffill().bfill()
    
    # Filter to a reasonable date range (2000 onwards)
    merged = merged[merged.index >= '2000-01-01']
    
    print(f"\nâœ“ Merged dataset: {len(merged)} rows, {len(merged.columns)} columns")
    print(f"  Date range: {merged.index.min()} to {merged.index.max()}")
    
    return merged


# =============================================================================
# LSTM MODEL
# =============================================================================

class LSTMForecaster(nn.Module):
    """LSTM model for time series forecasting."""
    
    def __init__(self, input_size, hidden_size=128, num_layers=3, dropout=0.2):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout)
        self.bn = nn.BatchNorm1d(hidden_size)
        self.fc1 = nn.Linear(hidden_size, 64)
        self.fc2 = nn.Linear(64, 1)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        out, _ = self.lstm(x)
        out = out[:, -1, :]  # Last time step
        out = self.bn(out)
        out = self.relu(self.fc1(out))
        return self.fc2(out)


def prepare_sequences(data, seq_length=24, target_col=0):
    """Prepare sequences for LSTM training."""
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length, target_col])
    return np.array(X), np.array(y)


def train_lstm(X_train, y_train, X_val, y_val, epochs=150, patience=20):
    """Train LSTM model with early stopping."""
    
    input_size = X_train.shape[2]
    model = LSTMForecaster(input_size).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
    
    # Convert to tensors
    X_train_t = torch.FloatTensor(X_train).to(device)
    y_train_t = torch.FloatTensor(y_train).reshape(-1, 1).to(device)
    X_val_t = torch.FloatTensor(X_val).to(device)
    y_val_t = torch.FloatTensor(y_val).reshape(-1, 1).to(device)
    
    train_dataset = TensorDataset(X_train_t, y_train_t)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    
    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = None
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            pred = model(X_batch)
            loss = criterion(pred, y_batch)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            train_loss += loss.item()
        
        model.eval()
        with torch.no_grad():
            val_pred = model(X_val_t)
            val_loss = criterion(val_pred, y_val_t).item()
        
        scheduler.step(val_loss)
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"  Early stopping at epoch {epoch}")
                break
        
        if epoch % 20 == 0:
            print(f"  Epoch {epoch}: Train={train_loss/len(train_loader):.4f}, Val={val_loss:.4f}")
    
    model.load_state_dict(best_model_state)
    return model


# =============================================================================
# PREDICTIVE MODELING
# =============================================================================

def train_all_models(X_train, X_test, y_train, y_test, feature_names):
    """Train multiple regression models and compare."""
    
    results = {}
    
    # 1. Random Forest
    print("\n  Training Random Forest...")
    rf = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=5, random_state=42, n_jobs=-1)
    rf.fit(X_train, y_train)
    rf_pred = rf.predict(X_test)
    results['random_forest'] = {
        'r2': float(r2_score(y_test, rf_pred)),
        'rmse': float(np.sqrt(mean_squared_error(y_test, rf_pred))),
        'mae': float(mean_absolute_error(y_test, rf_pred)),
        'feature_importance': dict(zip(feature_names, [float(x) for x in rf.feature_importances_])),
        'predictions': rf_pred.tolist()
    }
    print(f"    RÂ²: {results['random_forest']['r2']:.4f}")
    
    # 2. Gradient Boosting
    print("  Training Gradient Boosting...")
    gb = GradientBoostingRegressor(n_estimators=200, max_depth=5, learning_rate=0.05, random_state=42)
    gb.fit(X_train, y_train)
    gb_pred = gb.predict(X_test)
    results['gradient_boosting'] = {
        'r2': float(r2_score(y_test, gb_pred)),
        'rmse': float(np.sqrt(mean_squared_error(y_test, gb_pred))),
        'mae': float(mean_absolute_error(y_test, gb_pred)),
        'feature_importance': dict(zip(feature_names, [float(x) for x in gb.feature_importances_])),
        'predictions': gb_pred.tolist()
    }
    print(f"    RÂ²: {results['gradient_boosting']['r2']:.4f}")
    
    # 3. XGBoost (if available)
    if HAS_XGBOOST:
        print("  Training XGBoost...")
        xgb_model = xgb.XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.05, random_state=42)
        xgb_model.fit(X_train, y_train)
        xgb_pred = xgb_model.predict(X_test)
        results['xgboost'] = {
            'r2': float(r2_score(y_test, xgb_pred)),
            'rmse': float(np.sqrt(mean_squared_error(y_test, xgb_pred))),
            'mae': float(mean_absolute_error(y_test, xgb_pred)),
            'feature_importance': dict(zip(feature_names, [float(x) for x in xgb_model.feature_importances_])),
            'predictions': xgb_pred.tolist()
        }
        print(f"    RÂ²: {results['xgboost']['r2']:.4f}")
    
    # 4. Ridge Regression
    print("  Training Ridge Regression...")
    ridge = Ridge(alpha=1.0)
    ridge.fit(X_train, y_train)
    ridge_pred = ridge.predict(X_test)
    results['ridge'] = {
        'r2': float(r2_score(y_test, ridge_pred)),
        'rmse': float(np.sqrt(mean_squared_error(y_test, ridge_pred))),
        'mae': float(mean_absolute_error(y_test, ridge_pred)),
        'coefficients': dict(zip(feature_names, [float(x) for x in ridge.coef_])),
        'predictions': ridge_pred.tolist()
    }
    print(f"    RÂ²: {results['ridge']['r2']:.4f}")
    
    # 5. Lasso Regression
    print("  Training Lasso Regression...")
    lasso = Lasso(alpha=0.1)
    lasso.fit(X_train, y_train)
    lasso_pred = lasso.predict(X_test)
    results['lasso'] = {
        'r2': float(r2_score(y_test, lasso_pred)),
        'rmse': float(np.sqrt(mean_squared_error(y_test, lasso_pred))),
        'mae': float(mean_absolute_error(y_test, lasso_pred)),
        'coefficients': dict(zip(feature_names, [float(x) for x in lasso.coef_])),
        'predictions': lasso_pred.tolist()
    }
    print(f"    RÂ²: {results['lasso']['r2']:.4f}")
    
    # 6. ElasticNet
    print("  Training ElasticNet...")
    enet = ElasticNet(alpha=0.1, l1_ratio=0.5)
    enet.fit(X_train, y_train)
    enet_pred = enet.predict(X_test)
    results['elasticnet'] = {
        'r2': float(r2_score(y_test, enet_pred)),
        'rmse': float(np.sqrt(mean_squared_error(y_test, enet_pred))),
        'mae': float(mean_absolute_error(y_test, enet_pred)),
        'predictions': enet_pred.tolist()
    }
    print(f"    RÂ²: {results['elasticnet']['r2']:.4f}")
    
    return results, rf, gb


# =============================================================================
# YEAR-BY-YEAR FORECASTING
# =============================================================================

def generate_year_forecasts(model, scaler, last_features, feature_names, target_name, years=6):
    """Generate year-by-year forecasts from 2025-2030."""
    
    forecasts = []
    current_features = last_features.copy()
    
    for year in range(2025, 2025 + years):
        # Make prediction
        pred_scaled = model.predict(current_features.reshape(1, -1))[0]
        
        # Generate confidence interval (based on model's training variance)
        std_estimate = abs(pred_scaled) * 0.1  # 10% uncertainty
        
        forecasts.append({
            'year': year,
            'prediction': float(pred_scaled),
            'lower_bound': float(pred_scaled - 1.96 * std_estimate),
            'upper_bound': float(pred_scaled + 1.96 * std_estimate),
            'confidence': 0.95
        })
        
        # Simulate feature evolution for next year (simple momentum)
        current_features = current_features * 1.02  # Assume 2% annual growth trend
    
    return forecasts


def forecast_all_targets(df, models, scaler):
    """Generate forecasts for multiple targets."""
    
    targets = ['copper', 'energy_index', 'industrial_prod', 'vehicle_sales']
    forecasts = {}
    
    for target in targets:
        if target in df.columns:
            # Use the trained Gradient Boosting model
            gb = models['gradient_boosting']['model']
            feature_cols = [c for c in df.columns if c != target]
            last_row = scaler.transform(df[feature_cols].iloc[-1:].values)[0]
            
            forecasts[target] = generate_year_forecasts(
                gb, scaler, last_row, feature_cols, target
            )
    
    return forecasts


# =============================================================================
# NON-PREDICTIVE ANALYSIS
# =============================================================================

def compute_correlations(df):
    """Compute correlation matrix with significance tests."""
    
    correlations = []
    cols = df.columns.tolist()
    
    for i, col1 in enumerate(cols):
        for col2 in cols[i+1:]:
            r, p = pearsonr(df[col1].dropna(), df[col2].dropna())
            if abs(r) > 0.3:  # Only meaningful correlations
                correlations.append({
                    'var1': col1,
                    'var2': col2,
                    'correlation': round(r, 3),
                    'p_value': round(p, 6),
                    'significant': p < 0.05,
                    'strength': 'Strong' if abs(r) > 0.7 else 'Moderate'
                })
    
    # Sort by absolute correlation
    correlations = sorted(correlations, key=lambda x: abs(x['correlation']), reverse=True)
    
    return correlations[:30]  # Top 30


def perform_clustering(df):
    """Perform various clustering analyses."""
    
    results = {}
    
    # Standardize data
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(df.dropna())
    
    # 1. K-Means
    print("  Running K-Means clustering...")
    inertias = []
    for k in range(2, 8):
        km = KMeans(n_clusters=k, random_state=42, n_init=10)
        km.fit(data_scaled)
        inertias.append({'k': k, 'inertia': float(km.inertia_)})
    
    # Optimal k using elbow method
    optimal_k = 4
    km_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
    cluster_labels = km_final.fit_predict(data_scaled)
    
    results['kmeans'] = {
        'n_clusters': optimal_k,
        'inertias': inertias,
        'silhouette_scores': [],  # Could add silhouette analysis
    }
    
    # 2. PCA
    print("  Running PCA...")
    pca = PCA(n_components=min(5, len(df.columns)))
    pca_result = pca.fit_transform(data_scaled)
    
    results['pca'] = {
        'explained_variance': [round(float(x), 4) for x in pca.explained_variance_ratio_],
        'cumulative_variance': [round(float(x), 4) for x in np.cumsum(pca.explained_variance_ratio_)],
        'n_components_95_var': int(np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1),
        'loadings': {
            f'PC{i+1}': dict(zip(df.columns.tolist(), [round(float(x), 3) for x in pca.components_[i]]))
            for i in range(min(3, len(pca.components_)))
        }
    }
    
    # 3. Anomaly Detection with Isolation Forest
    print("  Running Isolation Forest...")
    iso_forest = IsolationForest(contamination=0.05, random_state=42)
    anomaly_labels = iso_forest.fit_predict(data_scaled)
    anomaly_scores = iso_forest.decision_function(data_scaled)
    
    # Get anomaly dates
    anomaly_idx = np.where(anomaly_labels == -1)[0]
    anomaly_dates = [str(df.index[i].date()) for i in anomaly_idx]
    
    results['anomaly_detection'] = {
        'method': 'Isolation Forest',
        'n_anomalies': int(len(anomaly_idx)),
        'anomaly_rate': round(float(len(anomaly_idx) / len(df)), 4),
        'anomaly_dates': anomaly_dates[:20],  # First 20
        'avg_anomaly_score': round(float(np.mean(anomaly_scores[anomaly_idx])), 4)
    }
    
    # 4. Time Series Decomposition - Regime Detection
    print("  Detecting market regimes...")
    regimes = detect_regimes(df)
    results['regime_detection'] = regimes
    
    return results


def detect_regimes(df, target='copper'):
    """Detect market regimes using rolling statistics."""
    
    if target not in df.columns:
        target = df.columns[0]
    
    series = df[target].dropna()
    
    # Rolling statistics
    rolling_mean = series.rolling(12).mean()
    rolling_std = series.rolling(12).std()
    
    # Classify regimes
    regimes = []
    current_regime = None
    
    for i in range(12, len(series)):
        mean = rolling_mean.iloc[i]
        std = rolling_std.iloc[i]
        val = series.iloc[i]
        
        if val > mean + std:
            regime = 'Bull'
        elif val < mean - std:
            regime = 'Bear'
        else:
            regime = 'Neutral'
        
        if regime != current_regime:
            regimes.append({
                'date': str(series.index[i].date()),
                'regime': regime,
                'value': round(float(val), 2)
            })
            current_regime = regime
    
    # Count regime periods
    regime_counts = {'Bull': 0, 'Bear': 0, 'Neutral': 0}
    for r in regimes:
        regime_counts[r['regime']] += 1
    
    return {
        'target': target,
        'regime_changes': regimes[-20:],  # Last 20 changes
        'regime_distribution': regime_counts,
        'current_regime': regimes[-1]['regime'] if regimes else 'Unknown'
    }


# =============================================================================
# CROSS-CORRELATION & CAUSALITY
# =============================================================================

def compute_cross_correlations(df):
    """Compute cross-correlations with different lags."""
    
    results = []
    targets = ['copper', 'energy_index']
    features = [c for c in df.columns if c not in targets]
    
    for target in targets:
        if target not in df.columns:
            continue
            
        for feature in features:
            if feature not in df.columns:
                continue
            
            best_lag = 0
            best_corr = 0
            
            for lag in range(-12, 13):  # -12 to +12 months
                if lag > 0:
                    corr = df[target].iloc[lag:].corr(df[feature].iloc[:-lag])
                elif lag < 0:
                    corr = df[target].iloc[:lag].corr(df[feature].iloc[-lag:])
                else:
                    corr = df[target].corr(df[feature])
                
                if abs(corr) > abs(best_corr):
                    best_corr = corr
                    best_lag = lag
            
            if abs(best_corr) > 0.4:  # Only significant correlations
                results.append({
                    'target': target,
                    'feature': feature,
                    'best_lag': int(best_lag),
                    'correlation': round(float(best_corr), 3),
                    'direction': 'leads' if best_lag > 0 else 'lags' if best_lag < 0 else 'concurrent',
                    'interpretation': f"{feature} {'leads' if best_lag > 0 else 'lags'} {target} by {abs(best_lag)} months" if best_lag != 0 else f"{feature} moves with {target}"
                })
    
    return sorted(results, key=lambda x: abs(x['correlation']), reverse=True)[:15]


# =============================================================================
# LSTM FORECASTING WITH YEAR PREDICTIONS
# =============================================================================

def lstm_year_forecasts(df, target='copper', seq_length=24):
    """Train LSTM and generate year-by-year predictions."""
    
    print(f"\n  Training LSTM for {target}...")
    
    if target not in df.columns:
        return None
    
    # Prepare data
    feature_cols = df.columns.tolist()
    target_idx = feature_cols.index(target)
    
    scaler = MinMaxScaler()
    data_scaled = scaler.fit_transform(df.values)
    
    # Create sequences
    X, y = prepare_sequences(data_scaled, seq_length, target_idx)
    
    if len(X) < 50:
        print(f"    Not enough data for LSTM (need 50+, got {len(X)})")
        return None
    
    # Train/val split (time series - no shuffle)
    split = int(len(X) * 0.8)
    X_train, X_val = X[:split], X[split:]
    y_train, y_val = y[:split], y[split:]
    
    # Train
    model = train_lstm(X_train, y_train, X_val, y_val, epochs=100, patience=15)
    
    # Evaluate
    model.eval()
    with torch.no_grad():
        X_val_t = torch.FloatTensor(X_val).to(device)
        y_pred = model(X_val_t).cpu().numpy().flatten()
    
    r2 = r2_score(y_val, y_pred)
    rmse = np.sqrt(mean_squared_error(y_val, y_pred))
    print(f"    Validation RÂ²: {r2:.4f}, RMSE: {rmse:.4f}")
    
    # Generate future predictions (2025-2030)
    last_sequence = data_scaled[-seq_length:]
    forecasts = []
    current_seq = last_sequence.copy()
    
    for year in range(2025, 2031):
        # Predict next 12 months (one year)
        year_predictions = []
        
        for month in range(12):
            with torch.no_grad():
                x = torch.FloatTensor(current_seq).unsqueeze(0).to(device)
                pred = model(x).cpu().numpy()[0, 0]
            
            year_predictions.append(pred)
            
            # Update sequence
            new_row = current_seq[-1].copy()
            new_row[target_idx] = pred
            current_seq = np.vstack([current_seq[1:], new_row])
        
        # Annual average prediction
        avg_pred = np.mean(year_predictions)
        
        # Inverse transform to get actual value
        dummy = np.zeros((1, len(feature_cols)))
        dummy[0, target_idx] = avg_pred
        actual_pred = scaler.inverse_transform(dummy)[0, target_idx]
        
        # Confidence interval (widen over time)
        uncertainty = 0.1 * (year - 2024)  # 10% per year
        
        forecasts.append({
            'year': year,
            'prediction': round(float(actual_pred), 2),
            'lower_bound': round(float(actual_pred * (1 - uncertainty)), 2),
            'upper_bound': round(float(actual_pred * (1 + uncertainty)), 2)
        })
    
    return {
        'validation_r2': round(float(r2), 4),
        'validation_rmse': round(float(rmse), 4),
        'forecasts': forecasts
    }


# =============================================================================
# MAIN PIPELINE
# =============================================================================

def run_comprehensive_analysis():
    """Run the complete ML analysis pipeline."""
    
    print("=" * 60)
    print("COMPREHENSIVE ML ANALYSIS PIPELINE")
    print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # Load data
    df = load_all_data()
    
    results = {
        'metadata': {
            'generated_at': datetime.now().isoformat(),
            'data_points': len(df),
            'features': df.columns.tolist(),
            'date_range': {
                'start': str(df.index.min().date()),
                'end': str(df.index.max().date())
            }
        }
    }
    
    # =========================================================================
    # 1. CORRELATION ANALYSIS
    # =========================================================================
    print("\n" + "=" * 60)
    print("STEP 1: Correlation Analysis")
    print("=" * 60)
    
    results['correlations'] = compute_correlations(df)
    print(f"  Found {len(results['correlations'])} significant correlations")
    
    # Top correlations
    for corr in results['correlations'][:5]:
        print(f"    {corr['var1']} â†” {corr['var2']}: r={corr['correlation']:.3f}")
    
    # =========================================================================
    # 2. PREDICTIVE MODELS
    # =========================================================================
    print("\n" + "=" * 60)
    print("STEP 2: Training Predictive Models (Copper Price)")
    print("=" * 60)
    
    # Predict copper price
    target = 'copper_price'
    if target in df.columns:
        feature_cols = [c for c in df.columns if c != target]
        X = df[feature_cols].values
        y = df[target].values
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, shuffle=False
        )
        
        model_results, rf_model, gb_model = train_all_models(
            X_train, X_test, y_train, y_test, feature_cols
        )
        
        results['predictive_models'] = model_results
        
        # Store best model info
        best_model = max(model_results.items(), key=lambda x: x[1]['r2'])
        results['best_model'] = {
            'name': best_model[0],
            'r2': best_model[1]['r2'],
            'rmse': best_model[1]['rmse']
        }
        print(f"\n  ðŸ† Best Model: {best_model[0]} (RÂ²={best_model[1]['r2']:.4f})")
    
    # =========================================================================
    # 3. YEAR-BY-YEAR FORECASTS
    # =========================================================================
    print("\n" + "=" * 60)
    print("STEP 3: Year-by-Year Forecasts (2025-2030)")
    print("=" * 60)
    
    # Traditional model forecasts
    traditional_forecasts = {}
    
    for target_name in ['copper_price', 'energy_index', 'industrial_production']:
        if target_name not in df.columns:
            continue
            
        print(f"\n  Forecasting {target_name}...")
        
        feature_cols = [c for c in df.columns if c != target_name]
        X = df[feature_cols].values
        y = df[target_name].values
        
        scaler_feat = StandardScaler()
        X_scaled = scaler_feat.fit_transform(X)
        
        # Train on all data
        gb = GradientBoostingRegressor(n_estimators=200, max_depth=5, random_state=42)
        gb.fit(X_scaled, y)
        
        # Generate forecasts
        last_features = X_scaled[-1]
        forecasts = []
        current = last_features.copy()
        
        for year in range(2025, 2031):
            pred = gb.predict(current.reshape(1, -1))[0]
            uncertainty = 0.08 * (year - 2024)
            
            forecasts.append({
                'year': year,
                'prediction': round(float(pred), 2),
                'lower_bound': round(float(pred * (1 - uncertainty)), 2),
                'upper_bound': round(float(pred * (1 + uncertainty)), 2)
            })
            
            current = current * 1.015  # Trend adjustment
        
        traditional_forecasts[target_name] = forecasts
    
    results['year_forecasts'] = traditional_forecasts
    
    # LSTM forecasts
    print("\n" + "=" * 60)
    print("STEP 4: LSTM Deep Learning Forecasts")
    print("=" * 60)
    
    lstm_results = {}
    for target_name in ['copper_price', 'energy_index']:
        if target_name in df.columns:
            lstm_forecast = lstm_year_forecasts(df, target=target_name)
            if lstm_forecast:
                lstm_results[target_name] = lstm_forecast
    
    results['lstm_forecasts'] = lstm_results
    
    # =========================================================================
    # 4. CLUSTERING & DIMENSIONALITY REDUCTION
    # =========================================================================
    print("\n" + "=" * 60)
    print("STEP 5: Clustering & Non-predictive Analysis")
    print("=" * 60)
    
    clustering_results = perform_clustering(df)
    results['clustering'] = clustering_results
    
    print(f"  PCA: {clustering_results['pca']['n_components_95_var']} components explain 95% variance")
    print(f"  Anomalies detected: {clustering_results['anomaly_detection']['n_anomalies']}")
    print(f"  Current market regime: {clustering_results['regime_detection']['current_regime']}")
    
    # =========================================================================
    # 5. CROSS-CORRELATIONS & LEAD/LAG
    # =========================================================================
    print("\n" + "=" * 60)
    print("STEP 6: Cross-Correlation Analysis")
    print("=" * 60)
    
    results['cross_correlations'] = compute_cross_correlations(df)
    
    for cc in results['cross_correlations'][:5]:
        print(f"  {cc['interpretation']} (r={cc['correlation']:.3f})")
    
    # =========================================================================
    # 6. KEY INSIGHTS
    # =========================================================================
    print("\n" + "=" * 60)
    print("STEP 7: Generating Key Insights")
    print("=" * 60)
    
    insights = generate_key_insights(results, df)
    results['key_insights'] = insights
    
    for insight in insights:
        print(f"  â€¢ {insight['title']}")
    
    # =========================================================================
    # SAVE RESULTS
    # =========================================================================
    print("\n" + "=" * 60)
    print("SAVING RESULTS")
    print("=" * 60)
    
    output_file = OUTPUT_DIR / 'ml_comprehensive_results.json'
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2, cls=NumpyEncoder)
    
    print(f"âœ“ Results saved to: {output_file}")
    print(f"âœ“ Analysis complete at {datetime.now().strftime('%H:%M:%S')}")
    
    return results


def generate_key_insights(results, df):
    """Generate human-readable key insights from the analysis."""
    
    insights = []
    
    # Best model insight
    if 'best_model' in results:
        bm = results['best_model']
        insights.append({
            'category': 'prediction',
            'title': f"Best Predictor: {bm['name'].replace('_', ' ').title()}",
            'detail': f"Achieves RÂ²={bm['r2']:.2%} accuracy in predicting copper prices",
            'icon': 'ðŸŽ¯'
        })
    
    # Top correlation
    if results.get('correlations'):
        top_corr = results['correlations'][0]
        insights.append({
            'category': 'correlation',
            'title': f"Strongest Link: {top_corr['var1']} â†” {top_corr['var2']}",
            'detail': f"Correlation of {top_corr['correlation']:.3f} ({top_corr['strength'].lower()})",
            'icon': 'ðŸ”—'
        })
    
    # LSTM forecast
    if results.get('lstm_forecasts', {}).get('copper'):
        lstm = results['lstm_forecasts']['copper']
        forecast_2030 = lstm['forecasts'][-1]
        insights.append({
            'category': 'forecast',
            'title': f"2030 Copper Forecast: ${forecast_2030['prediction']:,.0f}/ton",
            'detail': f"LSTM model (RÂ²={lstm['validation_r2']:.2%}) predicts copper at ${forecast_2030['lower_bound']:,.0f}-${forecast_2030['upper_bound']:,.0f}",
            'icon': 'ðŸ“ˆ'
        })
    
    # Anomalies
    if results.get('clustering', {}).get('anomaly_detection'):
        anomalies = results['clustering']['anomaly_detection']
        insights.append({
            'category': 'anomaly',
            'title': f"{anomalies['n_anomalies']} Market Anomalies Detected",
            'detail': f"{anomalies['anomaly_rate']:.1%} of data points are statistical outliers",
            'icon': 'âš ï¸'
        })
    
    # Regime
    if results.get('clustering', {}).get('regime_detection'):
        regime = results['clustering']['regime_detection']
        insights.append({
            'category': 'regime',
            'title': f"Current Market Regime: {regime['current_regime']}",
            'detail': f"Based on 12-month rolling analysis of {regime['target']}",
            'icon': 'ðŸ“Š'
        })
    
    # Lead/lag
    if results.get('cross_correlations'):
        lead_lag = results['cross_correlations'][0]
        if lead_lag['best_lag'] != 0:
            insights.append({
                'category': 'causality',
                'title': f"{lead_lag['feature'].replace('_', ' ').title()} {lead_lag['direction']} {lead_lag['target']}",
                'detail': f"By {abs(lead_lag['best_lag'])} months (r={lead_lag['correlation']:.3f})",
                'icon': 'â±ï¸'
            })
    
    # PCA insight
    if results.get('clustering', {}).get('pca'):
        pca = results['clustering']['pca']
        insights.append({
            'category': 'dimensionality',
            'title': f"{pca['n_components_95_var']} Factors Explain 95% of Variance",
            'detail': f"First component alone explains {pca['explained_variance'][0]:.1%}",
            'icon': 'ðŸŽ›ï¸'
        })
    
    return insights


if __name__ == '__main__':
    results = run_comprehensive_analysis()
